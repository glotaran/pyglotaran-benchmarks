{
    "BenchmarkOptimize.time_optimize": {
        "code": "class BenchmarkOptimize:\n    def time_optimize(self, index_dependent, grouped, weight):\n        optimize(self.scheme)\n\n    def setup(self, index_dependent, grouped, weight):\n        suite = MultichannelMulticomponentDecay\n        model = suite.model\n        # 0.4.0 API compat\n        model.is_grouped = grouped\n    \n        model.megacomplex[\"m1\"].is_index_dependent = index_dependent\n    \n        sim_model = suite.sim_model\n        suite.sim_model.megacomplex[\"m1\"].is_index_dependent = index_dependent\n    \n        wanted_parameters = suite.wanted_parameters\n    \n        initial_parameters = suite.initial_parameters\n        model.dataset[\"dataset1\"].fill(model, initial_parameters)\n    \n        if hasattr(suite, \"global_axis\"):\n            axes_dict = {\n                \"global\": getattr(suite, \"global_axis\"),\n                \"model\": getattr(suite, \"model_axis\"),\n            }\n        else:\n            # 0.4.0 API compat\n            axes_dict = {\n                \"e\": getattr(suite, \"e_axis\"),\n                \"c\": getattr(suite, \"c_axis\"),\n            }\n    \n        dataset = simulate(sim_model, \"dataset1\", wanted_parameters, axes_dict)\n    \n        if weight:\n            dataset[\"weight\"] = xr.DataArray(\n                np.ones_like(dataset.data) * 0.5, coords=dataset.data.coords\n            )\n    \n        data = {\"dataset1\": dataset}\n    \n        self.scheme = Scheme(\n            model=model,\n            parameters=initial_parameters,\n            data=data,\n            maximum_number_function_evaluations=10,\n            group_tolerance=0.1,\n            optimization_method=\"TrustRegionReflection\",\n        )\n        # 0.4.0 API compat\n        if hasattr(self.scheme, \"group\"):\n            self.scheme.group = grouped",
        "min_run_count": 2,
        "name": "BenchmarkOptimize.time_optimize",
        "number": 0,
        "param_names": [
            "index_dependent",
            "grouped",
            "weight"
        ],
        "params": [
            [
                "True",
                "False"
            ],
            [
                "True",
                "False"
            ],
            [
                "True",
                "False"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "timeout": 300,
        "type": "time",
        "unit": "seconds",
        "version": "1560762892c2d46b8de6b3fadeeb3450c70877cb113d447b9f710a6141cf83e5",
        "warmup_time": -1
    },
    "IntegrationTwoDatasets.peakmem_optimize": {
        "code": "class IntegrationTwoDatasets:\n    def peakmem_optimize(self):\n        optimize(self.scheme)\n\n    def setup(self):\n        dataset1 = load_dataset(SCRIPT_DIR / \"data/data1.ascii\")\n        dataset2 = load_dataset(SCRIPT_DIR / \"data/data2.ascii\")\n        parameters = load_parameters(str(SCRIPT_DIR / \"models/parameters.yml\"))\n        addition_kwargs = {}\n        if int(__version__.split(\".\")[1]) < 7:\n            model = load_model(str(SCRIPT_DIR / \"models/model_lt_0.7.0.yml\"))\n            addition_kwargs[\"non_negative_least_squares\"] = True\n        else:\n            model = load_model(str(SCRIPT_DIR / \"models/model.yml\"))\n        self.scheme = Scheme(\n            model,\n            parameters,\n            {\"dataset1\": dataset1, \"dataset2\": dataset2},\n            maximum_number_function_evaluations=11,\n            optimization_method=\"TrustRegionReflection\",\n            **addition_kwargs,\n        )",
        "name": "IntegrationTwoDatasets.peakmem_optimize",
        "param_names": [],
        "params": [],
        "timeout": 300,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "dd51208102b050544e77980718001a24ff24f8ec7de7a0ae92abdbbba7a45660"
    },
    "IntegrationTwoDatasets.time_optimize": {
        "code": "class IntegrationTwoDatasets:\n    def time_optimize(self):\n        optimize(self.scheme)\n\n    def setup(self):\n        dataset1 = load_dataset(SCRIPT_DIR / \"data/data1.ascii\")\n        dataset2 = load_dataset(SCRIPT_DIR / \"data/data2.ascii\")\n        parameters = load_parameters(str(SCRIPT_DIR / \"models/parameters.yml\"))\n        addition_kwargs = {}\n        if int(__version__.split(\".\")[1]) < 7:\n            model = load_model(str(SCRIPT_DIR / \"models/model_lt_0.7.0.yml\"))\n            addition_kwargs[\"non_negative_least_squares\"] = True\n        else:\n            model = load_model(str(SCRIPT_DIR / \"models/model.yml\"))\n        self.scheme = Scheme(\n            model,\n            parameters,\n            {\"dataset1\": dataset1, \"dataset2\": dataset2},\n            maximum_number_function_evaluations=11,\n            optimization_method=\"TrustRegionReflection\",\n            **addition_kwargs,\n        )",
        "min_run_count": 2,
        "name": "IntegrationTwoDatasets.time_optimize",
        "number": 0,
        "param_names": [],
        "params": [],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "timeout": 300,
        "type": "time",
        "unit": "seconds",
        "version": "8eab061a0b77c818415ddf03d1e998d6aeb84505258f3017b5e40f0f5bec3ca0",
        "warmup_time": -1
    },
    "version": 2
}